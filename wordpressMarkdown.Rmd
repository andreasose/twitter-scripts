---
title: "Using R for analysing tweets"
author: "Andreas Ose"
date: "1 June 2016"
output: html_document
---

Today I want to show how to use R (with some packages) in order to download and analyze tweets. Despite numerous such guides have been written before, I have been wanting to write this guide for several reasons:

- many of said guides only demonstrate how to download said tweets, and leave you to discover how to use them in any sensible way.
- As behavioural scientists, big data can be cool. More often however narrow hypotheses with carefully selected participants allows us to find more useful things, instead of shifting through a lot of noisy data.
- not enough of the scripts I found online are particularly reusable. Adding some simple functions to the scripts will help, as I've done in this post.
- There's really a lot of fun to be had with data from Twitter.
  
This guide requires that you have a Twitter account, R(studio), and have setup a twitter-dev account with OAuth. I'll leave the details of how to get OAuth here, since it explains it better than I would have been able to.  
By the end of this guide you will be able to download tweets from specific users and from lists, as well as plotting commonly used words and examining tweeting activity.
 


```{r}
#Packages required:
library(twitteR)
library(ggplot2)
library(httr)
library(rjson)
library(tm)
library(gridExtra)

```
If you don't have these packages, then use  `install.packages()` before running the code above. To get a feel for the commands we will pass through the code, I urge you to have a look at Twitter's [API documentation](https://dev.twitter.com/rest/public)
```{r, echo = F}
#Type in your app details from Twitter here:
setup_twitter_oauth(API_key, API_secret, access_token, access_secret)
```

## A starting point: The US election

To get warmed up, let's see what the two US presidential nominees have been talking about lately.
We will use the  httr package to request tweets from the API, and some functions from the twitteR package to help put all the tweets into a usable dataframe. All the commands passed to the API are specified in their documentation. Downloading tweets from a single user is very easy, just use the  userTimeline function from the  twitteR package
```{r}
#n specifies how many tweets you want from the user. We will use 200 in this example, but the maximum is 3200. Unfortunately you can't use the API to request tweets older than a week or two at most. This is a restriction of the Twitter Search API, and it often means you won't actually get the number of tweets you specified.
clinton_tweets <- userTimeline(user = "@HillaryClinton", n = 200, includeRts = FALSE, retryOnRateLimit = 2000)
trump_tweets <- userTimeline(user = "@realDonaldTrump", n = 200, includeRts = FALSE, retryOnRateLimit = 2000)

clinton_tweets <- twListToDF(clinton_tweets)
trump_tweets <- twListToDF(trump_tweets)

```

If you want to save these tweets for a later time, then use  write_csv() to write the tweets your hard drive.  
Now that we have the tweets down, it is useful to remove any web links and punctuation.



```{r}
# remove punctuation, numbers, html links and unecessary spaces:
textScrubber <- function(dataframe) {
    
    dataframe$text <-  gsub("&amp;", " ", dataframe$text)
    dataframe$text <-  gsub("[[:punct:]]", " ", dataframe$text)
    dataframe$text <-  gsub("[[:digit:]]", "", dataframe$text)
    dataframe$text <-  gsub("http\\w+", "", dataframe$text)
    dataframe$text <-  gsub("\n", " ", dataframe$text)
    dataframe$text <-  gsub("[ \t]{2,}", "", dataframe$text)
    dataframe$text <-  gsub("^\\s+|\\s+$", "", dataframe$text)
    dataframe$text <-  tolower(dataframe$text)
    
    return(dataframe)
}

```
To run the function, type in `clinton_tweets <- textScrubber(clinton_tweets)` and `trump_tweets <- textScrubber(trump_tweets)`  
After that, we remove all the so-called "stopwords" and convert the text into a Term Document Matrix. We can then count and plot the most used words by each nominee
```{r}
tdmCreator <- function(dataframe, term.freq = 10, stemDoc = F, rmStopwords = T){

    corpus <- Corpus(VectorSource(dataframe$text))
        if (isTRUE(rmStopwords)) {
            corpus <- tm_map(corpus, removeWords, stopwords())
        }
        if (isTRUE(stemDoc)) {
            corpus <- tm_map(corpus, stemDocument)
        }
    tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1,Inf)))
    termFreq <- rowSums(as.matrix(tdm))
    termFreq <- subset(termFreq, termFreq >= term.freq)
    df <- data.frame(term = names(termFreq), freq = termFreq)

    return(df)
}
```
This function works in the same way, but takes two extra arguments specifiying if you want to stem the words and remove any stopwords. I have set them to respectively false and true in this example for both `clinton_tweets <- tdmCreator(clinton_tweets)` and `trump_tweets <- tdmCreator(trump_tweets)`.  

Finally we can plot each of the candidates most used words:
```{r, fig.width= 11}
trump_plot <- ggplot(trump_tweets, aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = "identity", fill = "red") +
    xlab("Most Used") + ylab("How Often") +
    coord_flip() + theme(text=element_text(size=25,face="bold"))

clinton_plot <- ggplot(clinton_tweets, aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = "identity", fill = "blue") +
    xlab("Most Used") + ylab("How Often") +
    coord_flip() + theme(text=element_text(size=25,face="bold"))

grid.arrange(trump_plot, clinton_plot, ncol=2)

```
  
![image](https://raw.githubusercontent.com/andreasose/twitter-scripts/master/index2.png "Trump Vs Clinton")  

It should perhaps not come as any great surpise that Trump talks a lot about himself...
What's more interesting is how much Clinton talks about Trump! Also notice the subtle difference wording: Hillary talks about what we "can" do, wheras Trump says we "can't".
  
But that's only for two Twitter-accounts. What if we wanted to look at a selected sample or a section of a population? If you are just interested in massive amounts of data, then Twitter lets you sample 1% of their data. This is relatively noisy though, so often times it is much better (and smarter) to create a list of users you are interested in. After you have created your list you can simply download all the latest tweets from them using a simple for loop.  

To continue our investigation into the political world, let's compare tweets from [Democrat](https://twitter.com/TheDemocrats/lists/house-democrats) and [Republican](https://twitter.com/HouseGOP/lists/house-republicans) House Representatives  
To make things easier, I have created a function that allows us to extract tweets from all the members of a list. All you need to do is supply the function with the list owner, as well as the name of the list. For the democrats, the function would look like this: democrats <- tweetsFromList("TheDemocrats", "house-democrats")

```{r}

tweetsFromList <- function (listOwner, listName, sleepTime = 7, n_tweets = 200, includeRts = F) {

    api_url <- paste0("https://api.twitter.com/1.1/lists/members.json?slug=",
                  listName, "&owner_screen_name=", listOwner, "&count=5000")
    response <- GET(api_url, config(token=twitteR:::get_oauth_sig()))

    response_data <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
    ## If you want a list of their profile names, then remove the hastag underneath and
    ## add it to the return statement at the bottom (use a list).
    ## Useful if you need to verify identity of people.
    
    #user_title <- sapply(response_text$users, function(i) i$name)
    user_names <- sapply(response_data$users, function(i) i$screen_name)
    tweets <- list()
    ## Loops over list of users, use rbind() to add them to list.
    ## Sleeptime ticks inbetween to avoid rate limit.
        for (user in user_names) {
        ## Download user's timeline from Twitter  
        raw_data <- userTimeline(user, n = n_tweets,
                                 includeRts = includeRts,
                                 retryOnRateLimit = 2000)
        if (length(raw_data) != 0L) {
            tweets <- rbind(tweets, raw_data)
        print('Sleeping to avoid rate limit')
        Sys.sleep(sleepTime);
        }
            else {
                next
            }
         
        }
    rm(raw_data)
    tweets <- twListToDF(tweets)
    
    return(tweets)
}

```

For Democrats, the function would look like this: tweetsFromList("TheDemocrats", "house-democrats"). We can do the same for Republicans by calling republicans <- tweetsFromList("HouseGOP", "house-republicans") . Note that both of these commands will take some time as there will be a lot of tweets downloaded. To get an idea for how many tweets, use this formula: (n list members * 7)/60. There's 261 Republicans in our list, so that means it will take a minumum of 31 minutes to download all the tweets. If you want it to go faster you can adjust the sleepTime parameter. Note however that if you set it below 5, then the API will start throttling you. Go grab a drink instead.  
After that is done it is strongly advised that you save the contents to your hard drive (so you won't have to download them all at once again). Use write.csv(nameOfFile, "nameOfCSV.csv") to save it in your working directory.

All done? Excellent, now we have a lot of data to work with. Let's re-run the previous analysis (making sure to set term.freq paramater higher)
```{r}
#democrat <- read.csv("democrats.csv")
#republican <- read.csv("republicans.csv")
dem <- textScrubber(democrat)
rep <- textScrubber(republican)

dem <- tdmCreator(dem)
rep <- tdmCreator(rep)

```

![image](https://raw.githubusercontent.com/andreasose/twitter-scripts/master/RepVsDem.png)
  
  
That is certainly interesting. Turns out the House actually seems to care about the same things!