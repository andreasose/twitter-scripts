---
title: "Using R for analysing tweets"
author: "Andreas Ose"
date: "1 June 2016"
output: html_document
---

Today I want to show how to use R (with some packages) in order to download and analyze tweets. Despite numerous such guides have been written before, I have been wanting to write this guide for several reasons:

- many of said guides only demonstrate how to download said tweets, and leave you to discover how to use them in any sensible way.
- As behavioural scientists, big data can be cool. More often however narrow hypotheses with carefully selected participants allows us to find more useful things, instead of shifting through a lot of noisy data.
- not enough of the scripts I found online are particularly reusable. Adding some simple functions to the scripts will help, as I've done in this post.
- There's really a lot of fun to be had with data from Twitter.
  
This guide requires that you have a Twitter account, R(studio), and have setup a twitter-dev account with OAuth. I'll leave the details of how to get OAuth here, since it explains it better than I would have been able to.  
By the end of this guide you will be able to download tweets from specific users and from lists, as well as plotting commonly used words and examining tweeting activity.
 


```{r}
#Packages required:
library(twitteR)
library(ggplot2)
library(httr)
library(rjson)
library(tm)
library(gridExtra)

```
If you don't have these packages, then use  install.packages("twitteR", "ggplot2", "httr") before running the code above. To get a feel for the commands we will pass through the code, I urge you to have a look at Twitter's [API documentation](https://dev.twitter.com/rest/public)
```{r, echo = F}

#setup_twitter_oauth(API_key, API_secret, access_token, access_secret)
```

## A starting point: The US election

To get warmed up, let's see what the two US presidential nominees have been talking about lately.
We will use the  httr package to request tweets from the API, and some functions from the twitteR package to help put all the tweets into a usable dataframe. All the commands passed to the API are specified in their documentation. Downloading tweets from a single user is very easy, just use the  userTimeline function from the  twitteR package
```{r}
#n specifies how many tweets you want from the user. We will use 200 in this example, but the maximum is 3200. Unfortunately you can't use the API to request tweets older than a week or two at most. This is a restriction of the Twitter Search API, and it often means you won't actually get the number of tweets you specified.
#clinton_tweets <- userTimeline(user = "@HillaryClinton", n = 200, includeRts = FALSE, retryOnRateLimit = 2000)
#trump_tweets <- userTimeline(user = "@realDonaldTrump", n = 200, includeRts = FALSE, retryOnRateLimit = 2000)

#clinton_tweets <- twListToDF(clinton_tweets)
#trump_tweets <- twListToDF(trump_tweets)

#HASH THIS AWAY LATER:
clinton_tweets <- read.csv("clinton_tweets.csv")
trump_tweets <- read.csv("trump_tweets.csv")
```

If you want to save these tweets for a later time, then use  write_csv() to write the tweets your hard drive.  
Now that we have the tweets down, it is useful to remove any web links and punctuation, as well as stemming the document.



```{r}
# remove punctuation, numbers, html links and unecessary spaces:
textScrubber <- function(dataframe) {
    dataframe$text <- gsub("[[:punct:]]", "", dataframe$text)
    dataframe$text <-  gsub("[[:digit:]]", "", dataframe$text)
    dataframe$text <-  gsub("http\\w+", "", dataframe$text)
    dataframe$text <-  gsub("[ \t]{2,}", "", dataframe$text)
    dataframe$text <-  gsub("^\\s+|\\s+$", "", dataframe$text)
    return(dataframe)
}

```

After that, we remove all the so-called "stopwords" and convert the text into a Term Document Matrix. We can then count and plot the most used words by each nominee
```{r}
tdmCreator <- function(dataframe, term.freq = 10, stemDoc = F ){

corpus <- Corpus(VectorSource(dataframe$text))
corpus <- tm_map(corpus, removeWords, stopwords())
if (isTRUE(stemDoc)) {
    corpus <- tm_map(corpus, stemDocument)
}
tdm <- TermDocumentMatrix(corpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= term.freq)
df <- data.frame(term = names(term.freq), freq = term.freq)
return(df)
}
```
  
Finally we can plot each of the candidates most used words:
```{r, fig.width= 11}
trump_plot <- ggplot(trump_tweets, aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = "identity", fill = "red") +
    xlab("Most Used") + ylab("How Often") +
    coord_flip() + theme(text=element_text(size=25,face="bold"))

clinton_plot <- ggplot(clinton_tweets, aes(x = reorder(term, freq), y = freq)) +
    geom_bar(stat = "identity", fill = "blue") +
    xlab("Most Used") + ylab("How Often") +
    coord_flip() + theme(text=element_text(size=25,face="bold"))

grid.arrange(trump_plot, clinton_plot, ncol=2)

```
  
[image](https://raw.githubusercontent.com/andreasose/twitter-scripts/master/index2.png)  

It should perhaps not come as any great surpise that Trump talks a lot about himself...
What's more interesting is how much Clinton talks about Trump! Also notice the difference in Hillary in general being more positive: using the word "can" whereas Trump lambasts that we "can't"
  
But that's only for two Twitter-accounts. What if we wanted to look at a selected sample or a section of a population? If you are just interested in massive amounts of data, then Twitter lets you sample 1% of their data. This is relatively noisy though, so often times it is much better (and smarter) to create a list of users you are interested in. After you have created your list you can simply download all the latest tweets from them using a simple for loop.  
To continue our investigation into the political world, let's compare tweets from [Democrat](https://twitter.com/TheDemocrats/lists/house-democrats) and [Republican](https://twitter.com/HouseGOP/lists/house-republicans) House Representatives  
To make things easier, I have created a function that allows us to extract tweets from all the members of a list. All you need to do is supply the function with the list owner, as well as the name of the list. For the democrats, the function would look like this: democrats <- tweetsFromList("TheDemocrats", "house-democrats")

```{r}

tweetsFromList <- function (listOwner, listName, sleepTime = 7, n_tweets = 200, includeRts = F) {

    api_url <- paste0("https://api.twitter.com/1.1/lists/members.json?slug=",
                  listName, "&owner_screen_name=", listOwner, "&count=5000")
    response <- GET(api_url, config(token=twitteR:::get_oauth_sig()))

    response_data <- fromJSON(content(response, as = "text", encoding = "UTF-8"))
    ## If you want a list of their profile names, then remove the hastag underneath and
    ## add it to the return statement at the bottom.
    ## Useful if you need to verify identity of people.
    
    #user_title <- sapply(response_text$users, function(i) i$name)
    user_names <- sapply(response_data$users, function(i) i$screen_name)
    tweets <- list()
    
    ## Loops over list of users, use rbind() to add them to list.
    ## Sleeptime ticks inbetween to avoid rate limit.
        for (user in user_names) {
        ## Download user's timeline from Twitter  
        raw_data <- userTimeline(user, n = n_tweets,
                                 includeRts = includeRts,
                                 retryOnRateLimit = 2000)
        tweets <- rbind(tweets, raw_data)
        print('Sleeping to avoid rate limit')
        Sys.sleep(sleepTime); 
        }

rm(raw_data)
tweets <- twListToDF(tweets)
return(tweets)
}

```

We can do the same for Republicans by calling republicans <- tweetsFromList("HouseGOP", "house-republicans") . Note that both of these commands will take some time as there will be a lot of tweets downloaded. 
